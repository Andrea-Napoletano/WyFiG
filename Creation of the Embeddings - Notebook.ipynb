{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "import collections\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import Embeddings_Auxiliary_Functions as emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice: the user can either load a bipartite matrix, use the sample one, or generate a random matrix.\n",
    "#Alternatively, the user can skip this part and load a nested list directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE RANDOM TEST DATA OR LOAD USER DATA\n",
    "###################################################################################################################\n",
    "#Generate a random binary matrix that can be used as the adjacency matrix of the bipartite network.\n",
    "#example_data_matrix = sparse.random(1000, 100, density=0.1, data_rvs=np.ones)\n",
    "#example_data_matrix = example_data_matrix.toarray()\n",
    "#Use a fixed sample bipartite matrix\n",
    "\n",
    "#USE SAMPLE BIPARTITE MATRIX\n",
    "\n",
    "example_data_matrix = emb.load_sample_matrix()\n",
    "\n",
    "#Extract the nested list that the algoritm use as input\n",
    "example_data_nested_list = emb.extract_input_data(example_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATION OF THE EMBEDDINGS WITH THE SAMPLE DATA\n",
    "\n",
    "#Set the vocabulary_size, i.e. the number of words to embed\n",
    "#Notice that the default value of 500 depends on the size of the example_matrix\n",
    "#Adjust this value according to your data if you run the code on different data\n",
    "vocabulary_size = 500\n",
    "\n",
    "#Use the nested list as input to create the database for training Word2Vec\n",
    "data, reverse_dictionary, accumulated = emb.data_preprocessing(example_data_nested_list,vocabulary_size)\n",
    "\n",
    "#Define the number of iteration of the algorithm\n",
    "n_run = 60\n",
    "\n",
    "ListOfEmbeddings = []\n",
    "for cont_run in range(0,n_run):\n",
    "    print(\"realization number: \" +str(cont_run))\n",
    "    final_embeddings, codestoexp = emb.create_the_embeddings(data, reverse_dictionary, accumulated, vocabulary_size, num_steps=20000)\n",
    "    ListOfEmbeddings.append(final_embeddings)\n",
    "    \n",
    "List_of_Words = codestoexp\n",
    "\n",
    "#NOTICE: LAUNCING THIS FUNCTION WILL OVERWRITE THE EXISTING EMBEDDINGS UNLESS A DIFFERENT NAME TO SAVE THEM IS PROVIDED\n",
    "#emb.save_embeddings(ListOfEmbeddings,List_of_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the embedding tensor, shape: [n_run, vocabulary_size, embedding_size]\n",
    "EmbeddingTensor, ListOfWords = emb.load_embeddings()\n",
    "\n",
    "scalar_product_matrix = np.stack([np.dot(EmbeddingTensor[i],np.transpose(EmbeddingTensor[i])) for i in range(0,EmbeddingTensor.shape[0])])\n",
    "context_similarity_matrix = np.mean(scalar_product_matrix,axis = 0)\n",
    "cs_indexes = np.transpose(np.stack(np.nonzero(np.triu(context_similarity_matrix,k=1))))\n",
    "context_similarity = np.array([[item[0],item[1],context_similarity_matrix_0[item[0],item[1]]] for item in cs_indexes])\n",
    "    \n",
    "#Saving one istance of context similarity, specify a different name if you don't want to overwrite existing file\n",
    "emb.save_context_similarity(context_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell requires the default embeddings provided by the authors.\n",
    "#It creates different values of the context similarity and it calculates the average correlation between \n",
    "#the different realizations of context similarity to check the self correlation between different runs.\n",
    "#We find that the average correation is 0.958 +- 0.001\n",
    "\n",
    "# Recall that that \n",
    "n_randomization = 100 #number of randomization\n",
    "correlation_list = []\n",
    "for n in range(0,n_randomization):\n",
    "    #There are 60 sample embeddings so that two sets of context similarity can be calculated and their corellation can be studied\n",
    "    #Recal that context similarity is defined as the average over 30 runs of the scalar product between the embeddings\n",
    "    iteration_perm = np.random.choice(range(60),60,replace = False)\n",
    "    index_0 = iteration_perm[:30]\n",
    "    index_1 = iteration_perm[30:]\n",
    "\n",
    "    EmbeddingTensor_0 = EmbeddingTensor[index_0]\n",
    "    EmbeddingTensor_1 = EmbeddingTensor[index_1]\n",
    "\n",
    "    scalar_product_matrix_0 = np.stack([np.dot(EmbeddingTensor_0[i],np.transpose(EmbeddingTensor_0[i])) for i in range(0,30)])\n",
    "    scalar_product_matrix_1 = np.stack([np.dot(EmbeddingTensor_1[i],np.transpose(EmbeddingTensor_1[i])) for i in range(0,30)])\n",
    "\n",
    "    context_similarity_matrix_0 = np.mean(scalar_product_matrix_0,axis = 0)\n",
    "    context_similarity_matrix_1 = np.mean(scalar_product_matrix_1,axis = 0)\n",
    "    cs_indexes = np.transpose(np.stack(np.nonzero(np.triu(context_similarity_matrix_0,k=1))))\n",
    "    context_similarity_0 = np.array([[item[0],item[1],context_similarity_matrix_0[item[0],item[1]]] for item in cs_indexes])\n",
    "    cs_indexes = np.transpose(np.stack(np.nonzero(np.triu(context_similarity_matrix_1,k=1))))\n",
    "    context_similarity_1 = np.array([[item[0],item[1],context_similarity_matrix_1[item[0],item[1]]] for item in cs_indexes])\n",
    "\n",
    "    correlation_list.append(np.corrcoef(context_similarity_0[:,2],context_similarity_1[:,2])[0,1])\n",
    "    print(n,end=\"\\r\")\n",
    "    \n",
    "#Saving one istance of context similarity.\n",
    "#nb.save_context_similarity(context_similarity_0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_similarity = emb.load_context_similarity()\n",
    "plt.title(\"Sample Context Similarity Distribution\\nAverage Correlations: 0.958 +- 0.001\")\n",
    "plt.hist(context_similarity[:,2],bins = 40)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"context similarity\")\n",
    "plt.ylabel(\"N. Couples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu] *",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
